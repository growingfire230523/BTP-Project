{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "mmdZzKClg-wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/EPB_data (1).csv'  # Ensure the file is uploaded to Google Colab\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# 1. Attributes Mathematical Representation & Data Preprocessing (Heading 3.2)\n",
        "attributes_info = pd.DataFrame({\n",
        "    \"Data Type\": data.dtypes,\n",
        "    \"Unique Values\": data.nunique(),\n",
        "    \"Missing Values\": data.isnull().sum(),\n",
        "    \"Mean\": data.mean(numeric_only=True),\n",
        "    \"Standard Deviation\": data.std(numeric_only=True),\n",
        "    \"Minimum\": data.min(numeric_only=True),\n",
        "    \"Maximum\": data.max(numeric_only=True)\n",
        "})\n",
        "\n",
        "# Save Attributes Info\n",
        "attributes_info_path = \"/content/attributes_info.csv\"\n",
        "attributes_info.to_csv(attributes_info_path)\n",
        "print(f\"Attributes info saved at: {attributes_info_path}\")\n",
        "\n",
        "# 2. Statistical Summary\n",
        "statistical_summary = data.describe()\n",
        "\n",
        "# Save Statistical Summary\n",
        "statistical_summary_path = \"/content/statistical_summary.csv\"\n",
        "statistical_summary.to_csv(statistical_summary_path)\n",
        "print(f\"Statistical summary saved at: {statistical_summary_path}\")\n",
        "\n",
        "# 3. Box Plots\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, column in enumerate(data.columns, 1):\n",
        "    plt.subplot(3, 4, i)\n",
        "    sns.boxplot(data[column])\n",
        "    plt.title(column)\n",
        "plt.tight_layout()\n",
        "box_plots_path = \"/content/box_plots.png\"\n",
        "plt.savefig(box_plots_path)\n",
        "print(f\"Box plots saved at: {box_plots_path}\")\n",
        "plt.close()\n",
        "\n",
        "# 4. Density Plots\n",
        "plt.figure(figsize=(16, 10))\n",
        "for i, column in enumerate(data.columns, 1):\n",
        "    plt.subplot(3, 4, i)\n",
        "    sns.kdeplot(data[column], fill=True)\n",
        "    plt.title(column)\n",
        "plt.tight_layout()\n",
        "density_plots_path = \"/content/density_plots.png\"\n",
        "plt.savefig(density_plots_path)\n",
        "print(f\"Density plots saved at: {density_plots_path}\")\n",
        "plt.close()\n",
        "\n",
        "# 5. Correlation Metrics\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Save Correlation Metrics\n",
        "correlation_matrix_path = \"/content/correlation_matrix.csv\"\n",
        "correlation_matrix.to_csv(correlation_matrix_path)\n",
        "print(f\"Correlation metrics saved at: {correlation_matrix_path}\")\n",
        "\n",
        "# Generate Heatmap\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
        "correlation_heatmap_path = \"/content/correlation_heatmap.png\"\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.savefig(correlation_heatmap_path)\n",
        "print(f\"Correlation heatmap saved at: {correlation_heatmap_path}\")\n",
        "plt.close()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLCW_s3HpFV1",
        "outputId": "dcd69934-2949-4234-e92f-68a7a6926e4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attributes info saved at: /content/attributes_info.csv\n",
            "Statistical summary saved at: /content/statistical_summary.csv\n",
            "Box plots saved at: /content/box_plots.png\n",
            "Density plots saved at: /content/density_plots.png\n",
            "Correlation metrics saved at: /content/correlation_matrix.csv\n",
            "Correlation heatmap saved at: /content/correlation_heatmap.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "Zd5RqVMwhFox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Feature Engineering\n",
        "\n",
        "# Feature Engineering: Normalization and Standardization\n",
        "scaler = MinMaxScaler()\n",
        "data[['Relative Compactness', 'Surface Area', 'Wall Area', 'Roof Area']] = scaler.fit_transform(\n",
        "    data[['Relative Compactness', 'Surface Area', 'Wall Area', 'Roof Area']]\n",
        ")\n",
        "\n",
        "std_scaler = StandardScaler()\n",
        "data[['Heating Load', 'Cooling Load']] = std_scaler.fit_transform(data[['Heating Load', 'Cooling Load']])\n",
        "\n",
        "# Feature Engineering: Feature Interaction\n",
        "data['Surface x Glazing'] = data['Surface Area'] * data['Glazing Area']\n",
        "\n",
        "# Feature Engineering: Categorical Encoding\n",
        "encoder = OneHotEncoder(drop='first', sparse_output=False)  # Drop first to avoid multicollinearity, use sparse_output instead of sparse\n",
        "orientation_encoded = encoder.fit_transform(data[['Orientation']])\n",
        "glazing_distribution_encoded = encoder.fit_transform(data[['Glazing Area Distribution']])\n",
        "\n",
        "# Add encoded features to the dataset\n",
        "orientation_columns = [f'Orientation_{i+1}' for i in range(orientation_encoded.shape[1])]\n",
        "glazing_distribution_columns = [f'GlazingDist_{i+1}' for i in range(glazing_distribution_encoded.shape[1])]\n",
        "orientation_df = pd.DataFrame(orientation_encoded, columns=orientation_columns)\n",
        "glazing_dist_df = pd.DataFrame(glazing_distribution_encoded, columns=glazing_distribution_columns)\n",
        "data = pd.concat([data, orientation_df, glazing_dist_df], axis=1)\n",
        "\n",
        "# Drop original categorical features\n",
        "data.drop(['Orientation', 'Glazing Area Distribution'], axis=1, inplace=True)\n",
        "\n",
        "# Feature Engineering: Polynomial Features\n",
        "data['Overall Height^2'] = data['Overall Height'] ** 2\n",
        "\n",
        "# Feature Engineering: Domain-Specific Features\n",
        "data['Volume'] = data['Surface Area'] * data['Overall Height']\n",
        "data['Efficiency Index'] = (data['Heating Load'] + data['Cooling Load']) / data['Surface Area']\n",
        "\n",
        "# Feature Engineering: Correlation Analysis\n",
        "# Drop highly correlated features with correlation coefficient > 0.95\n",
        "correlation_threshold = 0.95\n",
        "corr_matrix = data.corr()\n",
        "to_drop = [column for column in corr_matrix.columns if any(corr_matrix[column] > correlation_threshold) and column != column]\n",
        "data.drop(columns=to_drop, inplace=True)\n",
        "\n",
        "# Save Feature Engineered Dataset\n",
        "feature_engineered_path = \"/content/feature_engineered_data.csv\"\n",
        "data.to_csv(feature_engineered_path, index=False)\n",
        "print(f\"Feature engineered dataset saved at: {feature_engineered_path}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TQWWhqomdKjj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dabc155b-dc76-47e9-c2a4-2b8499898463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineered dataset saved at: /content/feature_engineered_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Previous models"
      ],
      "metadata": {
        "id": "YZZtx_HghJPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "\n",
        "# Load the feature-engineered dataset\n",
        "feature_engineered_path = '/content/feature_engineered_data.csv'\n",
        "data = pd.read_csv(feature_engineered_path)\n",
        "\n",
        "# Define features (X) and targets (y)\n",
        "X = data.drop(columns=['Heating Load', 'Cooling Load'])  # Exclude target columns\n",
        "y_heating = data['Heating Load']\n",
        "y_cooling = data['Cooling Load']\n",
        "\n",
        "# Replace infinite values with NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Impute NaN values (e.g., with the mean of the column)\n",
        "for column in X.columns:\n",
        "    X[column].fillna(X[column].mean(), inplace=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train_h, y_test_h = train_test_split(X, y_heating, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train_c, y_test_c = train_test_split(X, y_cooling, test_size=0.2, random_state=42)\n",
        "\n",
        "# List of models to evaluate\n",
        "models = {\n",
        "    \"Extra Trees Regressor\": ExtraTreesRegressor(n_estimators=100, random_state=42),\n",
        "    \"XGBoost\": XGBRegressor(n_estimators=100, random_state=42),\n",
        "    \"Random Forest Regressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"LightGBM\": LGBMRegressor(n_estimators=100, random_state=42),\n",
        "    \"Gradient Boosting Regressor\": GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
        "    \"Decision Tree Regressor\": DecisionTreeRegressor(random_state=42)\n",
        "}\n",
        "\n",
        "# Function to calculate metrics\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    return {\n",
        "        \"Training\": {\n",
        "            \"R²\": r2_score(y_train, y_pred_train),\n",
        "            \"MAE\": mean_absolute_error(y_train, y_pred_train),\n",
        "            \"RMSE\": np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
        "        },\n",
        "        \"Testing\": {\n",
        "            \"R²\": r2_score(y_test, y_pred_test),\n",
        "            \"MAE\": mean_absolute_error(y_test, y_pred_test),\n",
        "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Evaluate each model for both heating and cooling loads\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    results[name] = {\n",
        "        \"Heating Load\": evaluate_model(model, X_train, X_test, y_train_h, y_test_h),\n",
        "        \"Cooling Load\": evaluate_model(model, X_train, X_test, y_train_c, y_test_c),\n",
        "    }\n",
        "\n",
        "# Print results\n",
        "for model_name, metrics in results.items():\n",
        "    print(f\"\\nModel: {model_name}\")\n",
        "    for load_type, performance in metrics.items():\n",
        "        print(f\"  {load_type}:\")\n",
        "        print(f\"    Training -> R²: {performance['Training']['R²']:.4f}, MAE: {performance['Training']['MAE']:.4f}, RMSE: {performance['Training']['RMSE']:.4f}\")\n",
        "        print(f\"    Testing  -> R²: {performance['Testing']['R²']:.4f}, MAE: {performance['Testing']['MAE']:.4f}, RMSE: {performance['Testing']['RMSE']:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nrSKQ3sMdV8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5a74be8-900d-4ba8-d69f-cace34d3df6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b8b3e635ad34>:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  X[column].fillna(X[column].mean(), inplace=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000067 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 306\n",
            "[LightGBM] [Info] Number of data points in the train set: 614, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score -0.015091\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000064 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 306\n",
            "[LightGBM] [Info] Number of data points in the train set: 614, number of used features: 18\n",
            "[LightGBM] [Info] Start training from score -0.018617\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "\n",
            "Model: Extra Trees Regressor\n",
            "  Heating Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0000, RMSE: 0.0000\n",
            "    Testing  -> R²: 0.9985, MAE: 0.0263, RMSE: 0.0398\n",
            "  Cooling Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0000, RMSE: 0.0000\n",
            "    Testing  -> R²: 0.9963, MAE: 0.0398, RMSE: 0.0616\n",
            "\n",
            "Model: XGBoost\n",
            "  Heating Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0020, RMSE: 0.0028\n",
            "    Testing  -> R²: 0.9987, MAE: 0.0242, RMSE: 0.0370\n",
            "  Cooling Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0033, RMSE: 0.0047\n",
            "    Testing  -> R²: 0.9954, MAE: 0.0401, RMSE: 0.0689\n",
            "\n",
            "Model: Random Forest Regressor\n",
            "  Heating Load:\n",
            "    Training -> R²: 0.9996, MAE: 0.0108, RMSE: 0.0210\n",
            "    Testing  -> R²: 0.9982, MAE: 0.0254, RMSE: 0.0426\n",
            "  Cooling Load:\n",
            "    Training -> R²: 0.9992, MAE: 0.0157, RMSE: 0.0282\n",
            "    Testing  -> R²: 0.9944, MAE: 0.0436, RMSE: 0.0756\n",
            "\n",
            "Model: LightGBM\n",
            "  Heating Load:\n",
            "    Training -> R²: 0.9973, MAE: 0.0270, RMSE: 0.0517\n",
            "    Testing  -> R²: 0.9959, MAE: 0.0384, RMSE: 0.0644\n",
            "  Cooling Load:\n",
            "    Training -> R²: 0.9956, MAE: 0.0397, RMSE: 0.0664\n",
            "    Testing  -> R²: 0.9893, MAE: 0.0646, RMSE: 0.1048\n",
            "\n",
            "Model: Gradient Boosting Regressor\n",
            "  Heating Load:\n",
            "    Training -> R²: 0.9981, MAE: 0.0299, RMSE: 0.0430\n",
            "    Testing  -> R²: 0.9974, MAE: 0.0371, RMSE: 0.0519\n",
            "  Cooling Load:\n",
            "    Training -> R²: 0.9948, MAE: 0.0491, RMSE: 0.0718\n",
            "    Testing  -> R²: 0.9903, MAE: 0.0705, RMSE: 0.0998\n",
            "\n",
            "Model: Decision Tree Regressor\n",
            "  Heating Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0000, RMSE: 0.0000\n",
            "    Testing  -> R²: 0.9976, MAE: 0.0282, RMSE: 0.0491\n",
            "  Cooling Load:\n",
            "    Training -> R²: 1.0000, MAE: 0.0000, RMSE: 0.0000\n",
            "    Testing  -> R²: 0.9906, MAE: 0.0460, RMSE: 0.0980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Genetic Algorithm"
      ],
      "metadata": {
        "id": "isVaAdSrhOep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "\n",
        "# Load the feature-engineered dataset\n",
        "feature_engineered_path = '/content/feature_engineered_data.csv'\n",
        "data = pd.read_csv(feature_engineered_path)\n",
        "\n",
        "# Define features (X) and targets (y)\n",
        "X = data.drop(columns=['Heating Load', 'Cooling Load'])\n",
        "y_heating = data['Heating Load']\n",
        "y_cooling = data['Cooling Load']\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train_h, y_test_h = train_test_split(X, y_heating, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train_c, y_test_c = train_test_split(X, y_cooling, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fitness Function\n",
        "def fitness_function(params, X, y, model_class):\n",
        "    \"\"\"Evaluate the model performance using a given set of hyperparameters.\"\"\"\n",
        "    model = model_class(**params)\n",
        "\n",
        "    # Create a copy of X to avoid modifying the original DataFrame\n",
        "    X_copy = X.copy()\n",
        "\n",
        "    # Check and handle infinite/NaN values in the copy of X\n",
        "    X_copy = X_copy.replace([np.inf, -np.inf], np.nan)  # Replace inf with NaN\n",
        "    X_copy = X_copy.fillna(X_copy.mean())  # Impute NaN with column means\n",
        "\n",
        "    try:\n",
        "        model.fit(X_copy, y)  # Fit the model using the copy\n",
        "        y_pred = model.predict(X_copy)  # Predict using the copy\n",
        "        return -r2_score(y, y_pred)  # Negative R² to minimize error\n",
        "    except ValueError as e:\n",
        "        if \"infinity or a value too large\" in str(e):\n",
        "            # Handle the error gracefully, e.g., return a very poor fitness score\n",
        "            return float('inf')\n",
        "        else:\n",
        "            raise  # Raise other ValueErrors\n",
        "\n",
        "# Genetic Algorithm Implementation\n",
        "def genetic_algorithm(\n",
        "    X, y, model_class, param_space, pop_size=10, generations=20, mutation_prob=0.1, crossover_prob=0.5\n",
        "):\n",
        "    # Initialize population with random parameters\n",
        "    population = [\n",
        "        {key: np.random.choice(values) for key, values in param_space.items()} for _ in range(pop_size)\n",
        "    ]\n",
        "    best_params = None\n",
        "    best_fitness = float('inf')\n",
        "\n",
        "    for generation in range(generations):\n",
        "        fitness_scores = [fitness_function(ind, X, y, model_class) for ind in population]\n",
        "        sorted_indices = np.argsort(fitness_scores)\n",
        "\n",
        "        # Keep track of the best solution\n",
        "        if fitness_scores[sorted_indices[0]] < best_fitness:\n",
        "            best_fitness = fitness_scores[sorted_indices[0]]\n",
        "            best_params = population[sorted_indices[0]]\n",
        "\n",
        "        # Select parents\n",
        "        parents = [population[i] for i in sorted_indices[:pop_size // 2]]\n",
        "\n",
        "        # Crossover\n",
        "        offspring = []\n",
        "        for _ in range(pop_size - len(parents)):\n",
        "            if np.random.rand() < crossover_prob:\n",
        "                parent1 = np.random.choice(parents)\n",
        "                parent2 = np.random.choice(parents)\n",
        "                child = {key: np.random.choice([parent1[key], parent2[key]]) for key in param_space.keys()}\n",
        "                offspring.append(child)\n",
        "\n",
        "        # Mutation\n",
        "        for child in offspring:\n",
        "            if np.random.rand() < mutation_prob:\n",
        "                key_to_mutate = np.random.choice(list(param_space.keys()))\n",
        "                child[key_to_mutate] = np.random.choice(param_space[key_to_mutate])\n",
        "\n",
        "        # Next generation\n",
        "        population = parents + offspring\n",
        "\n",
        "    return best_params, best_fitness\n",
        "\n",
        "# Define parameter space for Extra Trees Regressor\n",
        "param_space = {\n",
        "    'n_estimators': [50, 100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# # Run the genetic algorithm for heating load\n",
        "\n",
        "# Handle infinite/NaN values in X_train and X_test before model training\n",
        "X_train = X_train.replace([np.inf, -np.inf], np.nan)\n",
        "X_train = X_train.fillna(X_train.mean())\n",
        "\n",
        "X_test = X_test.replace([np.inf, -np.inf], np.nan)\n",
        "X_test = X_test.fillna(X_test.mean())\n",
        "\n",
        "# Run the genetic algorithm for heating load\n",
        "best_params_h, best_fitness_h = genetic_algorithm(X_train, y_train_h, ExtraTreesRegressor, param_space)\n",
        "\n",
        "# Run the genetic algorithm for cooling load\n",
        "best_params_c, best_fitness_c = genetic_algorithm(X_train, y_train_c, ExtraTreesRegressor, param_space)\n",
        "\n",
        "# ... (rest of the code) ...\n",
        "\n",
        "\n",
        "\n",
        "# Evaluate the performance of the best models\n",
        "def evaluate_model(X_train, X_test, y_train, y_test, model_params):\n",
        "    model = ExtraTreesRegressor(**model_params)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    return {\n",
        "        \"Training\": {\n",
        "            \"R²\": r2_score(y_train, y_pred_train),\n",
        "            \"MAE\": mean_absolute_error(y_train, y_pred_train),\n",
        "            \"RMSE\": np.sqrt(mean_squared_error(y_train, y_pred_train)),\n",
        "        },\n",
        "        \"Testing\": {\n",
        "            \"R²\": r2_score(y_test, y_pred_test),\n",
        "            \"MAE\": mean_absolute_error(y_test, y_pred_test),\n",
        "            \"RMSE\": np.sqrt(mean_squared_error(y_test, y_pred_test)),\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Performance metrics for heating load\n",
        "performance_h = evaluate_model(X_train, X_test, y_train_h, y_test_h, best_params_h)\n",
        "\n",
        "# Performance metrics for cooling load\n",
        "performance_c = evaluate_model(X_train, X_test, y_train_c, y_test_c, best_params_c)\n",
        "\n",
        "# Output results\n",
        "results = {\n",
        "    \"Heating Load\": {\n",
        "        \"Best Parameters\": best_params_h,\n",
        "        \"Performance\": performance_h\n",
        "    },\n",
        "    \"Cooling Load\": {\n",
        "        \"Best Parameters\": best_params_c,\n",
        "        \"Performance\": performance_c\n",
        "    }\n",
        "}\n",
        "print(results)\n"
      ],
      "metadata": {
        "id": "0hq3VPKRddNU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3ca526-ca14-46d0-fb99-2569f87455c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Heating Load': {'Best Parameters': {'n_estimators': 300, 'max_depth': 20, 'min_samples_split': 2, 'min_samples_leaf': 2}, 'Performance': {'Training': {'R²': 0.9996914815056265, 'MAE': 0.010600079814106085, 'RMSE': 0.017499312128429676}, 'Testing': {'R²': 0.9988601628381928, 'MAE': 0.023820461778985353, 'RMSE': 0.03418254079198293}}}, 'Cooling Load': {'Best Parameters': {'n_estimators': 200, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 1}, 'Performance': {'Training': {'R²': 0.9998254989889235, 'MAE': 0.008449831836184626, 'RMSE': 0.013156715688322114}, 'Testing': {'R²': 0.9963074369314525, 'MAE': 0.04116806392807455, 'RMSE': 0.06152549480297585}}}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "neWy4deJj-L0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}